{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process the datasets for masking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets, load_dataset, Dataset\n",
    "import os\n",
    "import re\n",
    "import json \n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence(sentence):\n",
    "    suffix = r'(\\$_\\S*)'\n",
    "    sentence = re.sub(suffix, '', sentence)\n",
    "    sentence = sentence.replace(\"$$\", \"\")\n",
    "    sentence = sentence.replace(\"[\", \"\")\n",
    "    sentence = sentence.replace(\"]\", \"\")\n",
    "    sentence = sentence.replace(\"  \", \" \")\n",
    "    suffix2 = r'_[^\\s]*'\n",
    "    sentence = re.sub(suffix2, '', sentence)\n",
    "    # remove spaces before punctuation\n",
    "    pattern = r'\\s+([.,;?!:])'\n",
    "    sentence = re.sub(pattern, r'\\1', sentence)\n",
    "    # remove weird ``\n",
    "    sentence = re.sub(r'``', '\"', sentence)\n",
    "    sentence = re.sub(r\"''\", '\"', sentence)\n",
    "    sentence = sentence.replace(\"\\/\", \"\")\n",
    "    return sentence\n",
    "\n",
    "def process_jsonl(input_file, output_file, col, gol):\n",
    "    with open(input_file, 'r', encoding='utf-8') as infile, \\\n",
    "         open(output_file, 'a', encoding='utf-8') as outfile:\n",
    "        \n",
    "        for line in infile:\n",
    "            data = json.loads(line)\n",
    "            data[col] = clean_sentence(data[col])\n",
    "            data[gol] = clean_sentence(data[gol])\n",
    "            outfile.write(json.dumps(data) + '\\n')\n",
    "\n",
    "def add_other_golds(input_file, output_file, sentcol, goldcol, finalgoldcol): # only tiger + tüba\n",
    "    with open(input_file, 'r', encoding='utf-8') as infile, \\\n",
    "         open(output_file, 'a', encoding='utf-8') as outfile:\n",
    "        for line in infile:\n",
    "            data = json.loads(line)\n",
    "            if data[goldcol] != \" \":\n",
    "                json.dump({sentcol: data[sentcol], finalgoldcol: data[goldcol], \"FCR\": data[\"FCR\"], \"Gapping\": data[\"Gapping\"], \"BCR\": data[\"BCR\"], \"SGF\": data[\"SGF\"]}, outfile)\n",
    "                outfile.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DELETE FILES BEFORE RUNNING THIS AGAIN!\n",
    "\n",
    "# print(\"Getting other gold standards!\")\n",
    "# print(\"Tiger Train\")\n",
    "# input_file = '/Users/marisa/data/tiger_train.jsonl'\n",
    "# output_file = '/Users/marisa/data/ALL_tiger_train.jsonl'\n",
    "\n",
    "# add_other_golds(input_file, output_file, \"Original sentence\", \"gold2 (LCO)\", \"Canonical form\")\n",
    "# add_other_golds(input_file, output_file, \"Original sentence\", \"Canonical form\", \"Canonical form\")\n",
    "\n",
    "# print(\"Tiger Test\")\n",
    "# input_file = '/Users/marisa/data/tiger_test.jsonl'\n",
    "# output_file = '/Users/marisa/data/ALL_tiger_test.jsonl'\n",
    "\n",
    "# add_other_golds(input_file, output_file, \"Original sentence\", \"gold2 (LCO)\", \"Canonical form\")\n",
    "# add_other_golds(input_file, output_file, \"Original sentence\", \"Canonical form\", \"Canonical form\")\n",
    "\n",
    "# print(\"TüBa Train\")\n",
    "# input_file = '/Users/marisa/data/tüba_train.jsonl'\n",
    "# output_file = '/Users/marisa/data/ALL_tüba_train.jsonl'\n",
    "\n",
    "# add_other_golds(input_file, output_file, \"Treebank-Sentence\", \"Gold_1\", \"Reconstructed-Sentence\")\n",
    "# add_other_golds(input_file, output_file, \"Treebank-Sentence\", \"Gold_2\", \"Reconstructed-Sentence\")\n",
    "# add_other_golds(input_file, output_file, \"Treebank-Sentence\", \"Gold_3\", \"Reconstructed-Sentence\")\n",
    "# add_other_golds(input_file, output_file, \"Treebank-Sentence\", \"Reconstructed-Sentence\", \"Reconstructed-Sentence\")\n",
    "# print(\"TüBa Test\")\n",
    "# input_file = '/Users/marisa/data/tüba_test.jsonl'\n",
    "# output_file = '/Users/marisa/data/ALL_tüba_test.jsonl'\n",
    "\n",
    "# add_other_golds(input_file, output_file, \"Treebank-Sentence\", \"Gold_1\", \"Reconstructed-Sentence\")\n",
    "# add_other_golds(input_file, output_file, \"Treebank-Sentence\", \"Gold_2\", \"Reconstructed-Sentence\")\n",
    "# add_other_golds(input_file, output_file, \"Treebank-Sentence\", \"Gold_3\", \"Reconstructed-Sentence\")\n",
    "# add_other_golds(input_file, output_file, \"Treebank-Sentence\", \"Reconstructed-Sentence\", \"Reconstructed-Sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3997\n",
      "3582\n",
      "2923\n",
      "1098\n",
      "Got train data\n"
     ]
    }
   ],
   "source": [
    "train_data1 = os.path.expanduser(\"~/data/ALL_tiger_train.jsonl\")\n",
    "train_data2 = os.path.expanduser(\"~/data/ALL_tüba_train.jsonl\")\n",
    "\n",
    "train_dataset1 = load_dataset(\"json\", data_files=train_data1, split='train')\n",
    "train_dataset2 = load_dataset(\"json\", data_files=train_data2, split='train')\n",
    "train_dataset2 = train_dataset2.rename_column(\"Treebank-Sentence\", \"Original sentence\")\n",
    "train_dataset2 = train_dataset2.rename_column(\"Reconstructed-Sentence\", \"Canonical form\")\n",
    "\n",
    "cols_to_check = ['BCR', 'FCR', 'Gapping', 'SGF']\n",
    "print(train_dataset1.num_rows)\n",
    "train_dataset1 = train_dataset1.filter(lambda row: not all(row[col] == \"0\" for col in cols_to_check))\n",
    "print(train_dataset1.num_rows)\n",
    "print(train_dataset2.num_rows)\n",
    "train_dataset2 = train_dataset2.filter(lambda row: not all(row[col] == \"0\" for col in cols_to_check))\n",
    "print(train_dataset2.num_rows)\n",
    "train_dataset = concatenate_datasets([train_dataset1, train_dataset2])\n",
    "print(\"Got train data\")\n",
    "\n",
    "t = \"Original sentence\"\n",
    "g = \"Canonical form\"\n",
    "batchsize = 4\n",
    "prefix = \"reconstruct the ellipsis in this sentence: \"\n",
    "epochs = 10 #5\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['Original sentence', 'Canonical form', 'FCR', 'Gapping', 'BCR', 'SGF'],\n",
      "    num_rows: 4680\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_extra_id(incomplete, complete):\n",
    "    incomplete_tokens = incomplete.split()\n",
    "    complete_tokens = complete.split()\n",
    "    \n",
    "    result = []\n",
    "    extra_id_counter = 0\n",
    "    i = 0  # Pointer for incomplete_tokens\n",
    "    j = 0  # Pointer for complete_tokens\n",
    "    while j < len(complete_tokens):\n",
    "        if i < len(incomplete_tokens) and incomplete_tokens[i] == complete_tokens[j]:\n",
    "            result.append(complete_tokens[j])\n",
    "            i += 1\n",
    "        else:\n",
    "            # Collect missing span until tokens match again\n",
    "            while j < len(complete_tokens) and (i >= len(incomplete_tokens) or complete_tokens[j] != incomplete_tokens[i]):\n",
    "                j += 1\n",
    "            result.append(f'<extra_id_{extra_id_counter}>')\n",
    "            extra_id_counter += 1\n",
    "            continue\n",
    "        j += 1\n",
    "    \n",
    "    return \" \".join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Masked': 'Sein Vorgänger habe keine Abfindung erhalten und <extra_id_0> beziehe auch bis zum Auslaufen seines Vertrages im Herbst kein Gehalt .', 'Target': 'Sein Vorgänger habe keine Abfindung erhalten und sein_f Vorgänger_f  beziehe auch bis zum Auslaufen seines Vertrages im Herbst kein Gehalt . '}\n"
     ]
    }
   ],
   "source": [
    "# Verarbeitung des gesamten Datensatzes\n",
    "processed_dataset = []\n",
    "for data in train_dataset:\n",
    "    incomplete = data['Original sentence']\n",
    "    complete = data['Canonical form']\n",
    "    processed_input = insert_extra_id(incomplete, complete)\n",
    "    \n",
    "    processed_dataset.append({\n",
    "        \"Masked\": processed_input,\n",
    "        \"Target\": complete\n",
    "    })\n",
    "\n",
    "# for entry in processed_dataset:\n",
    "#     print(\"Input with extra_id:\", entry['Masked'])\n",
    "#     print(\"Target:\", entry['Target'])\n",
    "#     print(\"---\")\n",
    "\n",
    "masked_dataset = Dataset.from_list(processed_dataset)\n",
    "print(masked_dataset[1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 4680/4680 [00:00<00:00, 40941.72 examples/s]\n",
      "Map: 100%|██████████| 4680/4680 [00:00<00:00, 38954.53 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Masked': 'Sein Vorgänger habe keine Abfindung erhalten und <extra_id_0> beziehe auch bis zum Auslaufen seines Vertrages im Herbst kein Gehalt .', 'Target': 'Sein Vorgänger habe keine Abfindung erhalten und sein Vorgänger beziehe auch bis zum Auslaufen seines Vertrages im Herbst kein Gehalt. '}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# CLEAN SENTENCES\n",
    "final_dataset = masked_dataset.map(lambda x: {'Masked': clean_sentence(x['Masked'])})\n",
    "final_dataset = masked_dataset.map(lambda x: {'Target': clean_sentence(x['Target'])})\n",
    "\n",
    "print(final_dataset[1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset = final_dataset.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map: 100%|██████████| 3744/3744 [00:00<00:00, 4728.64 examples/s]\n",
      "Map: 100%|██████████| 936/936 [00:00<00:00, 4724.03 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['Masked', 'Target', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 3744\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['Masked', 'Target', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 936\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 100/1404 [00:27<05:52,  3.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.9317, 'learning_rate': 4.643874643874644e-05, 'epoch': 0.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 200/1404 [00:54<05:27,  3.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3243, 'learning_rate': 4.287749287749288e-05, 'epoch': 0.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██▏       | 300/1404 [01:22<05:00,  3.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2617, 'learning_rate': 3.931623931623932e-05, 'epoch': 0.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 400/1404 [01:49<04:32,  3.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1835, 'learning_rate': 3.575498575498576e-05, 'epoch': 0.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 33%|███▎      | 468/1404 [02:15<04:14,  3.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.14146994054317474, 'eval_runtime': 7.6273, 'eval_samples_per_second': 122.718, 'eval_steps_per_second': 15.34, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 500/1404 [02:25<04:05,  3.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1518, 'learning_rate': 3.2193732193732194e-05, 'epoch': 1.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 600/1404 [02:52<03:39,  3.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1484, 'learning_rate': 2.863247863247863e-05, 'epoch': 1.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 700/1404 [03:19<03:10,  3.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.157, 'learning_rate': 2.5071225071225073e-05, 'epoch': 1.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 800/1404 [03:47<02:43,  3.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1417, 'learning_rate': 2.150997150997151e-05, 'epoch': 1.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 900/1404 [04:14<02:16,  3.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1551, 'learning_rate': 1.794871794871795e-05, 'epoch': 1.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 67%|██████▋   | 936/1404 [04:31<02:06,  3.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.12826012074947357, 'eval_runtime': 7.6035, 'eval_samples_per_second': 123.101, 'eval_steps_per_second': 15.388, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 1000/1404 [04:49<01:49,  3.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.167, 'learning_rate': 1.4387464387464389e-05, 'epoch': 2.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 1100/1404 [05:16<01:22,  3.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1124, 'learning_rate': 1.0826210826210826e-05, 'epoch': 2.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 1200/1404 [05:43<00:55,  3.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1214, 'learning_rate': 7.264957264957266e-06, 'epoch': 2.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 1300/1404 [06:10<00:28,  3.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1491, 'learning_rate': 3.7037037037037037e-06, 'epoch': 2.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 1400/1404 [06:38<00:01,  3.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1515, 'learning_rate': 1.4245014245014247e-07, 'epoch': 2.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      "100%|██████████| 1404/1404 [06:46<00:00,  3.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.12606953084468842, 'eval_runtime': 7.6005, 'eval_samples_per_second': 123.15, 'eval_steps_per_second': 15.394, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n",
      "100%|██████████| 1404/1404 [06:47<00:00,  3.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 407.8614, 'train_samples_per_second': 27.539, 'train_steps_per_second': 3.442, 'train_loss': 0.3676083913428491, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1404, training_loss=0.3676083913428491, metrics={'train_runtime': 407.8614, 'train_samples_per_second': 27.539, 'train_steps_per_second': 3.442, 'train_loss': 0.3676083913428491, 'epoch': 3.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizer laden\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "# Tokenisierung\n",
    "def tokenize_function(example):\n",
    "    inputs = tokenizer(example[\"Masked\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "    targets = tokenizer(example[\"Target\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "    \n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "# Tokenisiertes Dataset erstellen\n",
    "tokenized_dataset = final_dataset.map(tokenize_function, batched=True)\n",
    "print(tokenized_dataset)\n",
    "\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\").to(device)\n",
    "log_dir = os.path.expanduser(\"~/models/\" + \"FirstMasking\" + \"/logs\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"FirstMasking\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=log_dir,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False\n",
    ")\n",
    "\n",
    "# Trainer einrichten\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],  # Kann auch ein separater Validierungsdatensatz sein\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Training starten\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meine Schwester fährt ein rotes Auto und meine Schwester wohnt in Bayern\n",
      "Meine Schwester fährt ein rotes Auto und mein Schwester wohnt in Bayern\n",
      "Mein Schwester fährt ein rotes Auto und mein Schwester wohnt in Bayern\n",
      "Mein Schwester fährt ein rotes Auto und meine Schwester wohnt in Bayern\n",
      "Meine Schwester fährt ein rotes Auto und Meine Schwester wohnt in Bayern\n"
     ]
    }
   ],
   "source": [
    "def predict(model, tokenizer, input_text):\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=128, truncation=True).to(device)\n",
    "    outputs = model.generate(**inputs, num_beams=5, num_return_sequences=5)\n",
    "    return [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "\n",
    "test_input = \"Meine Schwester fährt ein rotes Auto und <extra_id_0> wohnt in Bayern.\"\n",
    "preds = predict(model, tokenizer, test_input)\n",
    "for pred in preds:\n",
    "    print(pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
