{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process the datasets for masking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets, load_dataset, Dataset\n",
    "import os\n",
    "import re\n",
    "import json \n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence(sentence):\n",
    "    suffix = r'(\\$_\\S*)'\n",
    "    sentence = re.sub(suffix, '', sentence)\n",
    "    sentence = sentence.replace(\"$$\", \"\")\n",
    "    sentence = sentence.replace(\"[\", \"\")\n",
    "    sentence = sentence.replace(\"]\", \"\")\n",
    "    sentence = sentence.replace(\"  \", \" \")\n",
    "    suffix2 = r'_[^\\s]*'\n",
    "    sentence = re.sub(suffix2, '', sentence)\n",
    "    # remove spaces before punctuation\n",
    "    pattern = r'\\s+([.,;?!:])'\n",
    "    sentence = re.sub(pattern, r'\\1', sentence)\n",
    "    # remove weird ``\n",
    "    sentence = re.sub(r'``', '\"', sentence)\n",
    "    sentence = re.sub(r\"''\", '\"', sentence)\n",
    "    sentence = sentence.replace(\"\\/\", \"\")\n",
    "    return sentence\n",
    "\n",
    "def process_jsonl(input_file, output_file, col, gol):\n",
    "    with open(input_file, 'r', encoding='utf-8') as infile, \\\n",
    "         open(output_file, 'a', encoding='utf-8') as outfile:\n",
    "        \n",
    "        for line in infile:\n",
    "            data = json.loads(line)\n",
    "            data[col] = clean_sentence(data[col])\n",
    "            data[gol] = clean_sentence(data[gol])\n",
    "            outfile.write(json.dumps(data) + '\\n')\n",
    "\n",
    "def add_other_golds(input_file, output_file, sentcol, goldcol, finalgoldcol): # only tiger + tüba\n",
    "    with open(input_file, 'r', encoding='utf-8') as infile, \\\n",
    "         open(output_file, 'a', encoding='utf-8') as outfile:\n",
    "        for line in infile:\n",
    "            data = json.loads(line)\n",
    "            if data[goldcol] != \" \":\n",
    "                json.dump({sentcol: data[sentcol], finalgoldcol: data[goldcol], \"FCR\": data[\"FCR\"], \"Gapping\": data[\"Gapping\"], \"BCR\": data[\"BCR\"], \"SGF\": data[\"SGF\"]}, outfile)\n",
    "                outfile.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DELETE FILES BEFORE RUNNING THIS AGAIN!\n",
    "\n",
    "# print(\"Getting other gold standards!\")\n",
    "# print(\"Tiger Train\")\n",
    "# input_file = '/Users/marisa/data/tiger_train.jsonl'\n",
    "# output_file = '/Users/marisa/data/ALL_tiger_train.jsonl'\n",
    "\n",
    "# add_other_golds(input_file, output_file, \"Original sentence\", \"gold2 (LCO)\", \"Canonical form\")\n",
    "# add_other_golds(input_file, output_file, \"Original sentence\", \"Canonical form\", \"Canonical form\")\n",
    "\n",
    "# print(\"Tiger Test\")\n",
    "# input_file = '/Users/marisa/data/tiger_test.jsonl'\n",
    "# output_file = '/Users/marisa/data/ALL_tiger_test.jsonl'\n",
    "\n",
    "# add_other_golds(input_file, output_file, \"Original sentence\", \"gold2 (LCO)\", \"Canonical form\")\n",
    "# add_other_golds(input_file, output_file, \"Original sentence\", \"Canonical form\", \"Canonical form\")\n",
    "\n",
    "# print(\"TüBa Train\")\n",
    "# input_file = '/Users/marisa/data/tüba_train.jsonl'\n",
    "# output_file = '/Users/marisa/data/ALL_tüba_train.jsonl'\n",
    "\n",
    "# add_other_golds(input_file, output_file, \"Treebank-Sentence\", \"Gold_1\", \"Reconstructed-Sentence\")\n",
    "# add_other_golds(input_file, output_file, \"Treebank-Sentence\", \"Gold_2\", \"Reconstructed-Sentence\")\n",
    "# add_other_golds(input_file, output_file, \"Treebank-Sentence\", \"Gold_3\", \"Reconstructed-Sentence\")\n",
    "# add_other_golds(input_file, output_file, \"Treebank-Sentence\", \"Reconstructed-Sentence\", \"Reconstructed-Sentence\")\n",
    "# print(\"TüBa Test\")\n",
    "# input_file = '/Users/marisa/data/tüba_test.jsonl'\n",
    "# output_file = '/Users/marisa/data/ALL_tüba_test.jsonl'\n",
    "\n",
    "# add_other_golds(input_file, output_file, \"Treebank-Sentence\", \"Gold_1\", \"Reconstructed-Sentence\")\n",
    "# add_other_golds(input_file, output_file, \"Treebank-Sentence\", \"Gold_2\", \"Reconstructed-Sentence\")\n",
    "# add_other_golds(input_file, output_file, \"Treebank-Sentence\", \"Gold_3\", \"Reconstructed-Sentence\")\n",
    "# add_other_golds(input_file, output_file, \"Treebank-Sentence\", \"Reconstructed-Sentence\", \"Reconstructed-Sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got train data\n",
      "Got test data\n"
     ]
    }
   ],
   "source": [
    "train_data1 = os.path.expanduser(\"~/data/ALL_tiger_train.jsonl\")\n",
    "train_data2 = os.path.expanduser(\"~/data/ALL_tüba_train.jsonl\")\n",
    "\n",
    "train_dataset1 = load_dataset(\"json\", data_files=train_data1, split='train')\n",
    "train_dataset2 = load_dataset(\"json\", data_files=train_data2, split='train')\n",
    "train_dataset2 = train_dataset2.rename_column(\"Treebank-Sentence\", \"Original sentence\")\n",
    "train_dataset2 = train_dataset2.rename_column(\"Reconstructed-Sentence\", \"Canonical form\")\n",
    "\n",
    "# cols_to_check = ['BCR', 'FCR', 'Gapping', 'SGF']\n",
    "# print(train_dataset1.num_rows)\n",
    "# train_dataset1 = train_dataset1.filter(lambda row: not all(row[col] == \"0\" for col in cols_to_check))\n",
    "# print(train_dataset1.num_rows)\n",
    "# print(train_dataset2.num_rows)\n",
    "# train_dataset2 = train_dataset2.filter(lambda row: not all(row[col] == \"0\" for col in cols_to_check))\n",
    "# print(train_dataset2.num_rows)\n",
    "train_dataset = concatenate_datasets([train_dataset1, train_dataset2])\n",
    "print(\"Got train data\")\n",
    "\n",
    "t = \"Original sentence\"\n",
    "g = \"Canonical form\"\n",
    "\n",
    "test_data1 = os.path.expanduser(\"~/data/ALL_tiger_test.jsonl\")\n",
    "test_data2 = os.path.expanduser(\"~/data/ALL_tüba_test.jsonl\")\n",
    "test_dataset1 = load_dataset(\"json\", data_files=test_data1, split='train')\n",
    "test_dataset2 = load_dataset(\"json\", data_files=test_data2, split='train')\n",
    "test_dataset2 = test_dataset2.rename_column(\"Treebank-Sentence\", \"Original sentence\")\n",
    "test_dataset2 = test_dataset2.rename_column(\"Reconstructed-Sentence\", \"Canonical form\")        \n",
    "test_dataset = concatenate_datasets([test_dataset1, test_dataset2])\n",
    "print(\"Got test data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['Original sentence', 'Canonical form', 'FCR', 'Gapping', 'BCR', 'SGF'],\n",
      "    num_rows: 6920\n",
      "})\n",
      "Dataset({\n",
      "    features: ['Original sentence', 'Canonical form', 'FCR', 'Gapping', 'BCR', 'SGF'],\n",
      "    num_rows: 1739\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_extra_id(incomplete, complete):\n",
    "    incomplete_tokens = incomplete.split()\n",
    "    complete_tokens = complete.split()\n",
    "    \n",
    "    result = []\n",
    "    extra_id_counter = 0\n",
    "    i = 0  # Pointer for incomplete_tokens\n",
    "    j = 0  # Pointer for complete_tokens\n",
    "    while j < len(complete_tokens):\n",
    "        if i < len(incomplete_tokens) and incomplete_tokens[i] == complete_tokens[j]:\n",
    "            result.append(complete_tokens[j])\n",
    "            i += 1\n",
    "        else:\n",
    "            # Collect missing span until tokens match again\n",
    "            while j < len(complete_tokens) and (i >= len(incomplete_tokens) or complete_tokens[j] != incomplete_tokens[i]):\n",
    "                j += 1\n",
    "            result.append(f'<extra_id_{extra_id_counter}>')\n",
    "            extra_id_counter += 1\n",
    "            continue\n",
    "        j += 1\n",
    "    \n",
    "    return \" \".join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Masked': 'Menschen , die dem schmutzigen Krieg Hitlers entfliehen wollten , qualifiziert Schreiber öffentlich so ab : Man müsse sehen , daß die Masse der Deserteure Leute waren , `` die sich entweder drücken wollten - der berühmte Drückeberger - <extra_id_0> , <extra_id_1>', 'Target': \"Menschen , die dem schmutzigen Krieg Hitlers entfliehen wollten , qualifiziert Schreiber öffentlich so ab : Man müsse sehen , daß die Masse der Deserteure Leute waren , `` die sich entweder drücken wollten - der berühmte Drückeberger - ,_g daß_g die_g Masse_g der_g Deserteure_g letzten Endes der Feigling waren_g , oder aber , und das ist viel wichtiger zu wissen ,_f daß_f die_f Masse_f der_f Deserteure_f Leute waren , die eine Strafverfolgung durch die Militärgerichte ... zu erwarten hatten , wegen ganz anderer Taten . '' \", 'FCR': '1', 'Gapping': '1', 'BCR': '0', 'SGF': '0'}\n",
      "{'Masked': 'Die Durchschnitts-Komödien , -Thriller und -Love Stories - warum sollte jemand dafür seine Wohnung verlassen und <extra_id_0> Geld ausgeben , wenn er sie frei Haus haben konnte ?', 'Target': 'Die Durchschnitts-Komödien , -Thriller und -Love Stories - warum sollte jemand dafür seine Wohnung verlassen und warum_fg sollte_fg jemand_fg dafür_fg Geld ausgeben , wenn er sie frei Haus haben konnte ? ', 'FCR': '0', 'Gapping': '0', 'BCR': '0', 'SGF': '0'}\n"
     ]
    }
   ],
   "source": [
    "# Verarbeitung des gesamten Datensatzes\n",
    "processed_dataset = []\n",
    "for data in train_dataset:\n",
    "    incomplete = data['Original sentence']\n",
    "    complete = data['Canonical form']\n",
    "    processed_input = insert_extra_id(incomplete, complete)\n",
    "    \n",
    "    processed_dataset.append({\n",
    "        \"Masked\": processed_input,\n",
    "        \"Target\": complete,\n",
    "        \"FCR\": data[\"FCR\"],\n",
    "        \"Gapping\": data[\"Gapping\"],\n",
    "        \"BCR\": data[\"BCR\"],\n",
    "        \"SGF\": data[\"SGF\"]\n",
    "    })\n",
    "\n",
    "# for entry in processed_dataset:\n",
    "#     print(\"Input with extra_id:\", entry['Masked'])\n",
    "#     print(\"Target:\", entry['Target'])\n",
    "#     print(\"---\")\n",
    "\n",
    "masked_dataset = Dataset.from_list(processed_dataset)\n",
    "print(masked_dataset[1000])\n",
    "\n",
    "processed_test_dataset = []\n",
    "for data in test_dataset:\n",
    "    incomplete = data['Original sentence']\n",
    "    complete = data['Canonical form']\n",
    "    processed_input = insert_extra_id(incomplete, complete)\n",
    "    \n",
    "    processed_test_dataset.append({\n",
    "        \"Masked\": processed_input,\n",
    "        \"Target\": complete,\n",
    "        \"FCR\": data[\"FCR\"],\n",
    "        \"Gapping\": data[\"Gapping\"],\n",
    "        \"BCR\": data[\"BCR\"],\n",
    "        \"SGF\": data[\"SGF\"]\n",
    "    })\n",
    "\n",
    "masked_test_dataset = Dataset.from_list(processed_test_dataset)\n",
    "print(masked_test_dataset[1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 6920/6920 [00:00<00:00, 17520.13 examples/s]\n",
      "Map: 100%|██████████| 6920/6920 [00:00<00:00, 22293.66 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Masked': 'Menschen , die dem schmutzigen Krieg Hitlers entfliehen wollten , qualifiziert Schreiber öffentlich so ab : Man müsse sehen , daß die Masse der Deserteure Leute waren , `` die sich entweder drücken wollten - der berühmte Drückeberger - <extra_id_0> , <extra_id_1>', 'Target': 'Menschen, die dem schmutzigen Krieg Hitlers entfliehen wollten, qualifiziert Schreiber öffentlich so ab: Man müsse sehen, daß die Masse der Deserteure Leute waren, \" die sich entweder drücken wollten - der berühmte Drückeberger -, daß die Masse der Deserteure letzten Endes der Feigling waren, oder aber, und das ist viel wichtiger zu wissen, daß die Masse der Deserteure Leute waren, die eine Strafverfolgung durch die Militärgerichte... zu erwarten hatten, wegen ganz anderer Taten. \" ', 'FCR': '1', 'Gapping': '1', 'BCR': '0', 'SGF': '0'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1739/1739 [00:00<00:00, 22129.74 examples/s]\n",
      "Map: 100%|██████████| 1739/1739 [00:00<00:00, 22823.59 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Masked': 'Die Durchschnitts-Komödien , -Thriller und -Love Stories - warum sollte jemand dafür seine Wohnung verlassen und <extra_id_0> Geld ausgeben , wenn er sie frei Haus haben konnte ?', 'Target': 'Die Durchschnitts-Komödien, -Thriller und -Love Stories - warum sollte jemand dafür seine Wohnung verlassen und warum sollte jemand dafür Geld ausgeben, wenn er sie frei Haus haben konnte? ', 'FCR': '0', 'Gapping': '0', 'BCR': '0', 'SGF': '0'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# CLEAN SENTENCES\n",
    "final_dataset = masked_dataset.map(lambda x: {'Masked': clean_sentence(x['Masked'])})\n",
    "final_dataset = masked_dataset.map(lambda x: {'Target': clean_sentence(x['Target'])})\n",
    "\n",
    "print(final_dataset[1000])\n",
    "\n",
    "final_test_dataset = masked_test_dataset.map(lambda x: {'Masked': clean_sentence(x['Masked'])})\n",
    "final_test_dataset = masked_test_dataset.map(lambda x: {'Target': clean_sentence(x['Target'])})\n",
    "\n",
    "print(final_test_dataset[1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 5536/5536 [00:00<00:00, 153867.39 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 1384/1384 [00:00<00:00, 164235.87 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 1739/1739 [00:00<00:00, 605754.89 examples/s]\n"
     ]
    }
   ],
   "source": [
    "final_dataset = final_dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "final_dataset.save_to_disk(\"MaskedTrainTestDataset\")\n",
    "final_test_dataset.save_to_disk(\"MaskedEvalDataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Masked': 'AbsolventInnen haben nach Abschluß ihres Studiums mehrere Blockpraktika in einschlägigen Betrieben und Institutionen hinter sich .', 'Target': 'AbsolventInnen haben nach Abschluß ihres Studiums mehrere Blockpraktika in einschlägigen Betrieben und Institutionen hinter sich.', 'FCR': '0', 'Gapping': '0', 'BCR': '0', 'SGF': '0'}\n"
     ]
    }
   ],
   "source": [
    "print(final_dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer laden\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "# Tokenisierung\n",
    "def tokenize_function(example):\n",
    "    inputs = tokenizer(example[\"Masked\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "    targets = tokenizer(example[\"Target\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "    \n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "# Tokenisiertes Dataset erstellen\n",
    "tokenized_dataset = final_dataset.map(tokenize_function, batched=True)\n",
    "print(tokenized_dataset)\n",
    "\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\").to(device)\n",
    "log_dir = os.path.expanduser(\"~/models/\" + \"FirstMasking\" + \"/logs\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"FirstMasking\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=log_dir,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False\n",
    ")\n",
    "\n",
    "# Trainer einrichten\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],  # Kann auch ein separater Validierungsdatensatz sein\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Training starten\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, tokenizer, input_text):\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=128, truncation=True).to(device)\n",
    "    outputs = model.generate(**inputs, num_beams=5, num_return_sequences=5)\n",
    "    return [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "\n",
    "test_input = \"Meine Schwester fährt ein rotes Auto und <extra_id_0> wohnt in Bayern.\"\n",
    "preds = predict(model, tokenizer, test_input)\n",
    "for pred in preds:\n",
    "    print(pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
